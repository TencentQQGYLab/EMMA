<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>DreamBooth</title>
  <style>
    .gif-row {
      display: flex;
      justify-content: center;
      /* Center the GIFs horizontally */
      align-items: center;
      /* Center the GIFs vertically if they have different heights */
      gap: 0px;
      /* Space between GIFs */
    }

    .gif-row img {
      max-width: 30%;
      /* Ensure the GIFs are responsive */
    }

    .gif-column {
      display: flex;
      flex-direction: column;
      align-items: center;
    }

    .gif-column img {
      margin: 0px 0;
      max-width: 30%;
      /* Ensure the GIFs are responsive */
    }
  </style>
  <link href="./DreamBooth_files/style.css" rel="stylesheet">
  <script type="text/javascript" src="./DreamBooth_files/jquery.mlens-1.0.min.js"></script>
  <script type="text/javascript" src="./DreamBooth_files/jquery.js"></script>
</head>

<!-- Yucheng Han1,2∗ Rui Wang2∗† Chi Zhang2∗ ‡ Juntao Hu2
Pei Cheng2 Bin Fu2 Hanwang Zhang -->
<!-- {yucheng002, hanwangzhang}@ntu.edu.sg
2 {raywwang, johnczhang, jetthu, brianfu}@tencent.com -->

<body>
  <div class="content">
    <h1><strong>EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts</strong></h1>
    <p id="authors"><span><a href="https://xxx.github.io/"></a></span><a
        href="https://tingxueronghua.github.io/">Yucheng Han*</a>
      <a href="https://wrong.wang/">Rui Wang*&dagger;
      </a> <a href="https://icoz69.github.io/">Chi Zhang*&loz;</a> <a>Juntao
        Hu</a> <a>Pei Cheng</a> <a>Bin
        Fu</a> <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang</a> <br>
      <br>
      <span style="font-size: 24px">Nanyang Technological University, Tencent
      </span>
      <br>
      <span style="font-size: 16px">*Equal contribution, &dagger; Project lead, &loz; Corresponding Author</span><br>
      <span style="font-size: 16px">You can contact Rui Wang for further details.</span><br>
    </p>
    <br>
    <!-- <img src="./DreamBooth_files/teaser_static.jpg" class="teaser-gif" style="width:100%;"> -->
    <div style="display: flex; justify-content: center; align-items: center;">
      <video id="teaser" controls="" loop="" playsinline="">
        <source src="./DreamBooth_files/video_teaser_480.mp4" type="video/mp4" codecs="avc1.42E01E, mp4a.40.2">
      </video>
    </div>
    <br>
    <!-- <h3 style="text-align:center"><em>It’s like a photo booth, but once the subject is captured, it can be synthesized
        wherever your dreams take you…</em></h3> -->
    <font size="+2">
      <!-- <p style="text-align: center;">
        <a href="https://arxiv.org/abs/2208.12242" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
        (<font color="#C70039">new!</font>) <a href="https://github.com/google/dreambooth" target="_blank">[Dataset]</a>
        &nbsp;&nbsp;&nbsp;&nbsp;
        <a href="DreamBooth_files/bibtex.txt" target="_blank">[BibTeX]</a>
      </p> -->
    </font>
  </div>
  <div class="content">
    <h2 style="text-align:center;">Abstract</h2>
    <p>Recent advancements in image generation have enabled the creation of high-quality images from text conditions.
      However, when facing multi-modal conditions, such as text combined with reference appearances, existing methods
      struggle to balance multiple conditions effectively, typically showing a preference for one modality over others.
      To address this challenge, we introduce EMMA, a novel image generation model accepting multi-modal prompts built
      upon the state-of-the-art text-to-image (T2I) diffusion model, ELLA. EMMA seamlessly incorporates additional
      modalities alongside text to guide image generation through an innovative Multi-modal Feature Connector design,
      which effectively integrates textual and supplementary modal information using a special attention mechanism. By
      freezing all parameters in the original T2I diffusion model and only adjusting some additional layers, we reveal
      an interesting finding that the pre-trained T2I diffusion model can secretly accept multi-modal prompts. This
      interesting property facilitates easy adaptation to different existing frameworks, making EMMA a flexible and
      effective tool for producing personalized and context-aware images and even videos. Additionally, we introduce a
      strategy to assemble learned EMMA modules to produce images conditioned on multiple modalities simultaneously,
      eliminating the need for additional training with mixed multi-modal prompts. Extensive experiments demonstrate the
      effectiveness of EMMA in maintaining high fidelity and detail in generated images, showcasing its potential as a
      robust solution for advanced multi-modal conditional image generation tasks.
    </p>
  </div>
  <!-- <div class="content">
    <h2>Background</h2>
    <p> Recent studies have explored image generation using multi-modal prompts, which require guidance from multiple
      sources. For instance, IP-Adapter uses both image prompts and textual instructions through cross-attention
      modules. Similarly, FaceStudio uses stylized images, facial images, and textual prompts for personalized portrait
      generation. These techniques have led to applications like subject-driven and personalized image generation, and
      artistic portrait creation.

      However, different strategies are employed for multi-modal prompts, and the general architecture for such image
      generation remains unclear. A key challenge is balancing various conditions during the generation process, as
      current methods may favor certain conditions. For example, IP-Adapter, which uses text and image features, often
      predominantly relies on image features due to model limitations. This results in a bias towards easier conditions
      and neglects more complex ones.

      Additionally, the scarcity of multi-modal training datasets in specialized domains exacerbates this issue. For
      subject-driven image generation, models like Kosmos-g and BootPig use cropped object images as conditions and
      ground truth, which can lead to a simple copy-paste functionality and ignore textual conditions. The lack of
      suitable training datasets and limitations in model architecture hinder achieving a balanced approach for image
      generation with multiple conditions.</p>
    <br>
  </div> -->
  <div class="content">
    <h2>Approach</h2>
    <p>Our proposed EMMA is built upon the state-of-the-art text-conditioned
      diffusion model ELLA, which trains a transformer-like module, named Perceiver Resampler, to
      connect text embeddings from pre-trained text encoders and pre-trained diffusion models for better text-guided
      image generation.
      ELLA has strong text-to-image generation ability, and our proposed EMMA could merge information from other
      modalities into text features for
      guidance. </p>
    <br>
    <img class="summary-img" src="./DreamBooth_files/emma_pipeline.png" style="width:100%;"> <br>
    <p>In detail, to control the image generation process by modalities beyond text, EMMA incorporates our proposed
      Assemblable Gated Perceiver Resampler (AGPR), which leverages cross-attention to inject information from
      additional modalities beyond texts. In our design, the AGPR blocks are strategically interleaved with the
      blocks
      of the Perceiver Resampler of ELLA. This arrangement ensures an effective integration of multi-modal
      information.
      During training, we freeze the raw modules of ELLA to maintain the control ability of text conditions.
    </p>
    <br>
    <img class="summary-img" src="./DreamBooth_files/emma_methods.png" style="width:80%;"> <br>
    <p>Notably, EMMA is inherently designed to handle multi-modal prompts as conditions, allowing for the
      straightforward combination of different multi-modal configurations. This is achieved by the gate mechanism in
      our
      AGPR, which could control the way of injecting information from other modalities into the textual features.
      This
      advantage enables diverse and complex inputs to be synthesized into a unified generation framework without the
      need for additional training.</p>
    <br><img class="summary-img" src="./DreamBooth_files/emma_ensemble_methods.png" style="width:80%;"> <br><br>
  </div>
  <div class="content">
    <h2>Image generation with different conditions
    </h2>
    <p>The generated images of EMMA under various conditions, including text-only, text + X, and text + composite
      conditions, showcase different individuals behaving differently in various scenes. Meanwhile, the details of the
      characters are preserved, ensuring the final results adhere to the text instructions.</p>
    <img class="summary-img" src="./DreamBooth_files/final_visualization.png" style="width:80%;">
  </div>
  <div class="content">
    <h2>Image Generation with text + portrait conditions
    </h2>
    <p>Here, we present additional images generated by EMMA under text + portrait conditions. Various portraits, each
      with unique features, adhere to the same prompts, demonstrating our model's excellent control over text
      conditioning and its ability to preserve individual identities.</p>
    <img class="summary-img" src="./DreamBooth_files/res_main.png" style="width:90%;">
  </div>
  <div class="content">
    <h2>Image Generation with text + portrait conditions using ToonYou</h2>
    <p>Given a text prompt and a portrait, our proposed EMMA can integrate with various diffusion models to generate
      images in different styles. Here are the images created using EMMA in conjunction with ToonYou.</p>
    <br>
    <img class="summary-img" src="./DreamBooth_files/res_toonyou.png" style="width:90%;"> <br>
  </div>
  <div class="content">
    <h2>Image Generation with text + portrait conditions using AnimateDiff</h2>
    <p>Given a portrait and a prompt, our proposed EMMA, combined with the AnimateDiff diffusion model, can generate
      images that preserve portrait details while adhering to text instructions.</p> <br>
    <div class="gif-row">
      <img src="./DreamBooth_files/woman_blue_dress.gif">
      <img src="./DreamBooth_files/woman_green_dress.gif">
      <img src="./DreamBooth_files/woman_purple_dress.gif">
    </div>
    <br>
    <!-- <img class="summary-img" src="./DreamBooth_files/res_video.png" style="width:90%;"> <br> -->
  </div>
  <div class="content">
    <h2>Story Telling</h2>
    <p>Images generated by our EMMA with portrait conditions. Two sets of images are generated for two separate stories.
      The first set of images is about a mailing woman chased by a dog. The second set of images is about a man finding
      treasures.</p>
    <br>
    <img class="summary-img" src="./DreamBooth_files/story_diffusion.png" style="width:90%;"> <br>
  </div>
  <!-- <div class="content">
    <h2>Societal Impact</h2>
    <p>This project aims to provide users with an effective tool for synthesizing personal subjects (animals,
      objects)
      in different contexts. While general text-to-image models might be biased towards specific attributes when
      synthesizing images from text, our approach enables the user to get a better reconstruction of their desirable
      subjects. On contrary, malicious parties might try to use such images to mislead viewers. This is a common
      issue,
      existing in other generative models approaches or content manipulation techniques. Future research in
      generative
      modeling, and specifically of personalized generative priors, must continue investigating and revalidating
      these
      concerns.</p>
    <br>
  </div> -->
  <!-- <div class="content">
    <h2>BibTex</h2>
    <code> @article{ruiz2022dreambooth,<br>
  &nbsp;&nbsp;title={DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation},<br>
  &nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2208.12242},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code>
  </div> -->
  <!-- <div class="content" id="acknowledgements">
    <p><strong>Acknowledgements</strong>:
      We thank Rinon Gal, Adi Zicher, Ron Mokady, Bill Freeman, Dilip Krishnan, Huiwen Chang and Daniel Cohen-Or for
      their valuable inputs that helped improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan
      for
      providing us with their support and the pretrained models of Imagen. Finally, a special thank you to David
      Salesin
      for his feedback, advice and for his support for the project.
    </p>
  </div> -->
</body>

</html>